{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of doctor-freud.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPXOJ0DrHnrWn5S/XmnvbMs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antodima/FisPro2Knime/blob/master/doctor_freud.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TL5YefYSe3N",
        "colab_type": "code",
        "outputId": "3b4beda2-68f3-461c-c894-4eb13b0084c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.13.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.86)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.16.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.1->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.1->boto3->transformers) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWNB7to6F3OL",
        "colab_type": "code",
        "outputId": "37fee85e-66bb-4076-8850-c5f2c06ebe08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!git clone https://antodima:Bulldog25041945@github.com/antodima/doctor-freud-qa.git"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'doctor-freud-qa' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYyy7QT0ZaQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%writefile setup.sh\n",
        "# git clone https://github.com/NVIDIA/apex\n",
        "# pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e9nnhDwZlIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!sh setup.sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tODv6gj-fo-",
        "colab_type": "code",
        "outputId": "fda15061-63fb-410f-f844-328b78a7080d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, Subset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import nltk\n",
        "from nltk import tokenize\n",
        "nltk.download('punkt')\n",
        "import numpy as np\n",
        "from time import time\n",
        "\n",
        "#from apex import amp\n",
        "from transformers import BertTokenizer, AdamW, BertModel, BertPreTrainedModel, get_linear_schedule_with_warmup\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BERT_MODEL_NAME = 'bert-base-uncased' #'bert-large-uncased-whole-word-masking-finetuned-squad'\n",
        "MAX_SENTENCE_SIZE = 50"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZvxIi3LGjsZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"/content/doctor-freud-qa/dataset/0_studies_on_hysteria.txt\") as file:\n",
        "  data_0 = file.read().replace('\\n', '')\n",
        "with open(\"/content/doctor-freud-qa/dataset/1_the_interpretation_of_dreams.txt\") as file:\n",
        "  data_1 = file.read().replace('\\n', '')\n",
        "with open(\"/content/doctor-freud-qa/dataset/2_totem_and_taboo.txt\") as file:\n",
        "  data_2 = file.read().replace('\\n', '')\n",
        "with open(\"/content/doctor-freud-qa/dataset/3_jokes_and_their_relation_to_the_unconscious.txt\") as file:\n",
        "  data_3 = file.read().replace('\\n', '')\n",
        "with open(\"/content/doctor-freud-qa/dataset/4_reflections_on_war_and_death.txt\") as file:\n",
        "  data_4 = file.read().replace('\\n', '')\n",
        "with open(\"/content/doctor-freud-qa/dataset/5_the_psychopathology_of_everyday_life.txt\") as file:\n",
        "  data_5 = file.read().replace('\\n', '')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3wGJzwoIdpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = []\n",
        "for sentence in tokenize.sent_tokenize(data_0):\n",
        "  dataset.append({ 'sentence': sentence, 'class_id': 0, 'label': 'studies on hysteria' })\n",
        "for sentence in tokenize.sent_tokenize(data_1):\n",
        "  dataset.append({ 'sentence': sentence, 'class_id': 1, 'label': 'the interpretation of dreams' })\n",
        "for sentence in tokenize.sent_tokenize(data_2):\n",
        "  dataset.append({ 'sentence': sentence, 'class_id': 2, 'label': 'totem and taboo' })\n",
        "for sentence in tokenize.sent_tokenize(data_3):\n",
        "  dataset.append({ 'sentence': sentence, 'class_id': 3, 'label': 'jokes and their relation to the unconscious' })\n",
        "for sentence in tokenize.sent_tokenize(data_4):\n",
        "  dataset.append({ 'sentence': sentence, 'class_id': 4, 'label': 'reflections on war and death' })\n",
        "for sentence in tokenize.sent_tokenize(data_5):\n",
        "  dataset.append({ 'sentence': sentence, 'class_id': 5, 'label': 'the psychopathology of everyday life' })"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aa91EAEwcVkU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FreudDataset(Dataset):\n",
        "\n",
        "  def __init__(self, sentences, transform=None):\n",
        "    self.sentences = sentences\n",
        "    self.len = len(self.sentences)\n",
        "    self.transform = transform\n",
        "    self.tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.len\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    item = self.sentences[index]\n",
        "    class_id = item['class_id']\n",
        "    sentence = item['sentence']\n",
        "\n",
        "    tokenized_sentence = self.tokenizer.encode(sentence, add_special_tokens=True)\n",
        "    if len(tokenized_sentence) < MAX_SENTENCE_SIZE:\n",
        "      padded_tokenized_sentence = np.array(tokenized_sentence + [0]*(MAX_SENTENCE_SIZE-len(tokenized_sentence)))\n",
        "    else:\n",
        "      padded_tokenized_sentence = np.array(tokenized_sentence[0:MAX_SENTENCE_SIZE])\n",
        "    attention_mask  = np.where(padded_tokenized_sentence != 0, 1, 0)\n",
        "\n",
        "    input_ids = padded_tokenized_sentence\n",
        "    return torch.tensor(input_ids), torch.tensor(attention_mask), torch.tensor(class_id)\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "  print('batch',batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j1SO6pDfcOX",
        "colab_type": "code",
        "outputId": "d426cb2b-9151-4ab0-dc7a-d3efc0bc8b78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "trainset = FreudDataset(dataset[0:int(len(dataset)*0.8)]) # 80% of dataset\n",
        "trainset_loader = DataLoader(trainset, batch_size=10, shuffle=True, num_workers=1, drop_last=True)\n",
        "testset = FreudDataset(dataset[int(len(dataset)*0.8):], transform=transforms.ToTensor()) # 20% of dataset\n",
        "testset_loader = DataLoader(testset, batch_size=1000, shuffle=False, num_workers=1)\n",
        "print('Training set size:', len(trainset))\n",
        "print('Test set size:', len(testset))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set size: 20771\n",
            "Test set size: 5193\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-DhkYBdSRjO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertForQuestionAnswering(BertPreTrainedModel):\n",
        "  \n",
        "  def __init__(self, config):\n",
        "        super(BertForQuestionAnswering, self).__init__(config)\n",
        "        self.bert = BertModel(config)\n",
        "        self.qa_outputs = nn.Linear(config.hidden_size, 2)  # start/end\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "        self.init_weights()\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n",
        "        outputs = self.bert(input_ids,\n",
        "                            attention_mask=attention_mask,\n",
        "                            token_type_ids=token_type_ids,\n",
        "                            position_ids=position_ids, \n",
        "                            head_mask=head_mask)\n",
        "        sequence_output = outputs[0]\n",
        "        pooled_output = outputs[1]\n",
        "        # predict start & end position\n",
        "        qa_logits = self.qa_outputs(sequence_output)\n",
        "        start_logits, end_logits = qa_logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "        # classification\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        classifier_logits = self.classifier(pooled_output)\n",
        "        return start_logits, end_logits, classifier_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNFoL0l4YNQF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_fn(preds, labels):\n",
        "    start_preds, end_preds, class_preds = preds\n",
        "    start_labels, end_labels, class_labels = labels\n",
        "    \n",
        "    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n",
        "    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n",
        "    class_loss = nn.CrossEntropyLoss()(class_preds, class_labels)\n",
        "    return start_loss + end_loss + class_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvXNcgDcYRW1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BertForQuestionAnswering.from_pretrained(BERT_MODEL_NAME, num_labels=6)\n",
        "model = model.to(device)\n",
        "model.zero_grad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DofM3iExgaf3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 2e-5\n",
        "n_epochs = 1\n",
        "optimizer = AdamW(model.parameters(), lr=lr, correct_bias=False)\n",
        "\n",
        "def train(model, epoch, log_interval=100):\n",
        "    model.train()  # set training mode\n",
        "    iteration = 0\n",
        "    for ep in range(epoch):\n",
        "        start = time()\n",
        "        for batch_idx, (x, m, y) in enumerate(trainset_loader):\n",
        "            x_batch, mask_batch, y_batch = x, m, y\n",
        "            y_batch = (y.to(device) for y in y_batch)\n",
        "            y_pred = model(x_batch.to(device), attention_mask=mask_batch.to(device))\n",
        "            loss = loss_fn(y_pred, y_batch)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if iteration % log_interval == 0:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    ep, batch_idx * len(data), len(trainset_loader.dataset),\n",
        "                    100. * batch_idx / len(trainset_loader), loss.item()))\n",
        "            iteration += 1\n",
        "            \n",
        "        end = time()\n",
        "        print('{:.2f}s'.format(end-start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvBw9vQyhNmE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(model, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}